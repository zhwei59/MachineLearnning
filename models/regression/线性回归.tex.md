## 回归分析
回归的任务是生成一个方程来描述多个预测变量和响应变量之间的关系，并预测新的观测值，线性回归一般是通过残差平方和最小化导出方程的普通最小二乘法来求解最优解。

## 线性回归
作为最基础的模型，线性回归值得深入学习。
假设数据有房子面积和年份，用线性回归来拟合：就会得到这样的关系式：
$y=\theta_1+x_1\theta_2+x_2\theta$

####寻找目标函数
扩展到一般向量形式：
$y= \sum\limits_{i=1}^{n} \theta_ix_i=\theta^Tx$
任何数据都存在误差，因此关系式应该引入误差$\varepsilon$于是关系式变成：
$ y_i=\theta^T x_i+\varepsilon_i $
于是得到y的分布：
$y = \sum\limits_{i=1}^{n}\theta^Tx_i+\sum\limits_{i=1}^n\varepsilon_i$

这里的$\varepsilon$服从正态分布。根据中心极限定理，那么残差必定满足均值为0，方差为$\sigma^2$的正态分布：

$p(\varepsilon_i)=\frac {1}  {\sqrt{2\pi\sigma}} exp(-\frac {\varepsilon^2} {2\sigma^2}) $

将前式的$\varepsilon$和y的关系带入：

$p(y^{(i)}|x;\theta)=\frac {1} {\sqrt{2\pi\sigma}} exp(-\frac {y^{(i)}-\theta^Tx} {2\sigma^2})$

对样本进行求似然L得到似然方程：
$ mlog\frac {1} {\sqrt(2\pi\sigma))}-\frac {1} {\sigma^2}\cdot \frac {1} {2} \sum\limits_{i=1}^n(y^{(i)}-\theta^Tx)$

终于得到了损失函数：
$J(\theta)=\frac {1}{2}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$


#### 最优化
对似然函数的极大值等于损失函数的极小值。

分别用最小二乘法和梯度下降求解极小值。

####最小二乘法
将损失函数写成向量的形式
$J(\theta)=\frac {1}{2}(X{\theta}-y)^T(X{\theta}-y)$

令梯度等于0，这时候j就可以$\theta$的显示解
$\theta=(X^TX)^{-1}(X^Ty)$

#### 梯度下降
找到最陡方向，下降。

我们要在这个函数里面找到它的最小值。那么我们可以把这个函数的图像想象成两座大山，假如我们的初始值设定在左边这座山的话，我们要做的就是原地先旋转360度，看看我们周围我要在哪个方向是最陡的，可以让我尽快下山的。每走一步，你就旋转360度，朝最快下山的方向迈进一步。就这样重复走下去，直到你走到局部最低的位置。

引入学习率来改进梯度下降算法，学习率调整下降的速度。有必要每步都调整学习率吗？其实没有必要，因为接近山底的时候，固定的学习率，步长也会变小。


## 广义线性回归
回归的一般思路：
符合指数族分布的一般模型 ====》转化成广义线性模型====》求解

想用 广义线性模型对一般问题进行建模首先需要明确3个 假设：

- y的条件概率分布符合 指数族分布。
- 拟合的h(x)=E(y|x)。
- 自然参数和x是线性关系。



## 为什么要引入GLM

回归就是预测，线性回归的输出值是连续的，所以线性回归其实做不了分类，但是线性模型简单，效果不错的特性非常吸引人。为了保持使用线性回归，就引入了连接函数的方法。GLM的模型也变成了下面的方式：

样本=====》线性回归===》连续值======》连接函数=====》离散值或者连续值

$h_\theta(x)$称为连接函数，为什么普通线性回归也归入广义线性回归呢？因为线性回归的 $h_\theta(x)$的表达可以看成$f=x$也算是一种转换。这里有点绕，连接函数是这个过程的反函数。在这一步，是将$\eta$作为输入的，而将$h_\theta(x)$作为输出的。

## 连接函数
并不是所有函数都能成为连接函数。在广义线性模型下，限定了$X,Y$的分布关系符合指数族分布。
$P(Y,\eta)=B(Y){\ast}e^{\eta{\ast}T(Y)-B(n)}$
其中$\eta$是上面提到的回归模型输出的连续值，也就是说$\eta=\theta^TX$

有了上述条件，满足指数族分布的X,Y其连接函数可以通过$E(T(Y))=h_\theta(x)$来求解。

事实上，指数族分布b包括了除了柯西分布和t分布外的基本分布，所以适用性非常广。

以$h_\theta(x)$为函数输入，$\eta$为函数输出，的这对映射关系称为连接函数。这是定义，连接函数必须是$h_{\theta}(x)$为自变量，而$\eta$为因变量。求连接函数的步骤：

1、把Y,X所满足的指数族分布写成指数族分布的形式

2、根据形式得出T(Y),$\eta$向量值。

3、算出E(T(Y))和$\eta$的关系，并把$\theta^TX$带入$\eta$中，得到$h_\theta(x)$的表达式。

## 常见连接函数的推导


## 高斯分布假设---线性回归
对符合高斯分布的p(y)进行变形，得到广义线性模型。

## 伯努利假设----逻辑回归


## softmax推导----多分类


## 岭回归

## lasso回归