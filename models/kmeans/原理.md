## Kmeans原理

### 前提假设
数据之间的相似度用欧氏距离量衡量，如果不是欧氏距离，要转化成欧氏距离。

余弦相似度，欧氏距离和杰卡德相似度
- 余弦相似度强调的是方向，两者的夹角越接近0度，越相似，考虑的趋势。
- 欧氏距离强调的是空间点之间的距离，所以虽然两者方向上可能一致，但是距离相差甚远。
- 余弦相似度用在用户兴趣表示上，欧氏距离用在用户价值表示上。
- 杰卡德相似度强调的是集合中的同现度。杰卡德距离通过两个集合中不同元素的比例来表示区分度。

### 算法讲解
k-means是无监督模型，将样本按照给定的k进行分类。
### 伪代码

```
随机生成k个质心
while 不收敛:
    遍历data:
        遍历旧质心:
            生成新坐标
    计算i和质心的距离
    生成新的质心
    是否收敛
为了不陷入局部最优的情况，采用最小的距离衡量多次训练，生成最优质心。

```


## 存在的问题
1.陷入局部最小值
2.孤独的质点
3.收敛
4.权重


### sklearn 的kmeans
sklearn的kmeans有两种，普通的KMeans,采样的MiniBatchKMeans类。


## k值的确立
K值确定

法1：(轮廓系数)在实际应用中，由于Kmean一般作为数据预处理，或者用于辅助分聚类贴标签。所以k一般不会设置很大。可以通过枚举，令k从2到一个固定值如10，在每个k值上重复运行数次kmeans(避免局部最优解)，并计算当前k的平均轮廓系数，最后选取轮廓系数最大的值对应的k作为最终的集群数目。

法2：(Calinski-Harabasz准则)

